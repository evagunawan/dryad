#!/usr/bin/env python3
#Author: Kelsey Florek
#email: kelsey.florek@slh.wisc.edu
#description: A pipeline for constructing SNP based and  core gene set reference free phylogenies

import subprocess as sub
import sys, os, argparse
import docker
import multiprocessing as mp
from shutil import copyfile

#local libraries
from app.lib import cpu_count, checkexists
from app.lib import StatusTracker as ST
from app.trimming import q_trim
from app.core_genome import core_genome

#setup argparser to display help if no arguments
class MyParser(argparse.ArgumentParser):
    def error(self, message):
        sys.stderr.write('error: %s\n' % message)
        self.print_help()
        sys.exit(2)

#determine command line arguments and get path
parser = MyParser(description='A pipeline for constructing SNP based and reference free phylogenies.')
subparsers= parser.add_subparsers(title='required arguments',metavar='pipeline',dest="subparser_name",required=True)
#core geneome pipeline
parser_cg = subparsers.add_parser('cg', help='reference free core geneome pipeline')
parser_cg.add_argument('reads', type=str,help="text file listing the location of paired reads to be included in the analysis")
parser_cg.add_argument('-o',metavar='output', type=str,help="output directory - defaults to working directory")
parser_cg.add_argument('-t',metavar='threads', type=int,help="number of cpus to use for pipeline",default=4)
#snp pipeline
parser_snp = subparsers.add_parser('snp', help='CFSAN SNP pipeline')
parser_snp.add_argument('reads', type=str,help="text file listing the location of paired reads to be included in the analysis")
parser_snp.add_argument('reference_sequence', type=str,help="reference fasta for SNP tree")
parser_snp.add_argument('-o',metavar='output', type=str,help="output directory - defaults to working directory")
parser_snp.add_argument('-t',metavar='threads', type=int,help="number of cpus to use for pipeline",default=4)

args = parser.parse_args()

pipeline = ''
if args.subparser_name == 'cg':
    pipeline = args.subparser_name

if args.subparser_name == 'snp':
    pipeline = args.subparser_name
    #get reference path
    reference = os.path.abspath(args.reference_sequence)

#common args
threads = args.t
reads_path = args.reads
#get current working dir if output is empty
try:
    out = os.path.abspath(args.o)
except (AttributeError, TypeError) as err:
    out = os.getcwd()

#open file and pull locations into a list
with open(reads_path,'r') as f:
    r_list = []
    for line in f:
        if line.strip() != '':
            r_list.append(line.strip())

#sort and join pairs
r_list.sort()
if len(r_list) % 2 != 0:
    print('There is an uneven number of read pairs in the read list. Exiting.')
    sys.exit()
paired_reads = []
[paired_reads.append([x,y]) for x,y in zip(r_list[0::2],r_list[1::2])]

#get num of jobs and number of cpus per job
jobs,cpu_job = cpu_count(threads)

#initialize tracker
tracker = ST()
outdir = tracker.initialize(out,pipeline)

if not tracker.check_status('trimmed'):
    #start trimming
    q_trim(paired_reads,jobs,cpu_job,outdir,tracker)

if tracker.check_status('cg'):
    #start core-genome pipeline
    core_genome(jobs,cpu_job,outdir,tracker)

if tracker.check_status('snp'):
    #start snp pipeline
    pass
